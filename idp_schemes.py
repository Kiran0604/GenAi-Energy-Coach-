# -*- coding: utf-8 -*-
"""IDP Schemes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15z-beZ_SnzaOZQ1Kqj66iJIgkTExqKBi
"""

!pip install -q langchain-groq langchain chromadb sentence-transformers unstructured
!pip install -U langchain-community
!pip install -U langchain
!pip install -U httpx
!pip install groq
!pip install -U langchain langchain-chroma

from langchain_chroma import Chroma
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
import os

# Explicitly set the API key in the environment variable
os.environ["GROQ_API_KEY"] = "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"

# Get the API key (double-check it)
groq_api_key = os.getenv("GROQ_API_KEY")

if not groq_api_key:
    raise ValueError("Groq API Key is not set properly.")

# ‚úÖ STEP 1: Install all required packages (once)
!pip install -q langchain langchain-community langchain-groq pymupdf

# ‚úÖ STEP 2: Imports
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader  # ‚úÖ Correct import
from langchain.text_splitter import RecursiveCharacterTextSplitter

import os

# ‚úÖ STEP 3: Set and verify your Groq API Key
os.environ["GROQ_API_KEY"] = "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"
groq_api_key = os.getenv("GROQ_API_KEY")

if not groq_api_key:
    raise ValueError("Groq API Key is not set properly.")

# ‚úÖ STEP 4: Initialize LLM
llm = ChatGroq(api_key=groq_api_key, model_name="llama3-8b-8192")

# Setup
pdf_folder_path = "/content/drive/MyDrive/Schemes"
persist_directory = "/content/drive/MyDrive/Schemes_DB"
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

import shutil
from google.colab import files

# Path to your Chroma DB directory
persist_directory = "/content/drive/MyDrive/Schemes_DB"

# Output zip file name
zip_filename = "Schemes_DB.zip"

# Create zip file
shutil.make_archive("Schemes_DB", 'zip', persist_directory)

# Download zip file in Colab
files.download(zip_filename)

# ‚úÖ Install required packages
!pip install -q langchain sentence-transformers chromadb

# ‚úÖ Step 1: Import required libraries
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from google.colab import files
import zipfile
import os

# ‚úÖ Step 2: Upload zipped vector DB
print("üìÅ Please upload your zipped Chroma Vector DB...")
uploaded = files.upload()

# ‚úÖ Step 3: Extract uploaded ZIP file
zip_name = list(uploaded.keys())[0]
unzip_path = "/content/uploaded_vectordb"
os.makedirs(unzip_path, exist_ok=True)

with zipfile.ZipFile(zip_name, 'r') as zip_ref:
    zip_ref.extractall(unzip_path)

print(f"‚úÖ Extracted Vector DB to: {unzip_path}")

# ‚úÖ Step 4: Load embedding model
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ‚úÖ Step 5: Load vectorstore from extracted directory
vectorstore = Chroma(
    persist_directory=unzip_path,
    embedding_function=embedding
)

# ‚úÖ Step 6: Create retriever
retriever = vectorstore.as_retriever()

print("‚úÖ Vector DB loaded successfully and retriever is ready.")

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Define prompt template
prompt_template = PromptTemplate(
    input_variables=["question", "context"],
    template=(
        "You are an AI-powered virtual assistant dedicated to providing support and guidance about "
        "energy-related government schemes, subsidies, and incentives specifically for Indian MSMEs "
        "in the industrial sector.\n\n"
        "You are strictly limited to answering questions ONLY about:\n"
        "- Government schemes, subsidies, or incentives for industrial MSMEs\n"
        "- Energy efficiency programs for industrial MSMEs\n"
        "- Renewable energy initiatives and adoption for industrial MSMEs\n"
        "- Regulatory frameworks or policies affecting industrial MSME energy use\n"
        "- Energy-saving recommendations for industrial MSMEs\n\n"
        "Do NOT answer questions about non-industrial, household, agricultural, or commercial energy topics, "
        "or about MSMEs outside the industrial/manufacturing sector. If the question is outside this scope, "
        "simply respond with: 'Sorry, I can only answer questions related to energy schemes, incentives, or recommendations "
        "for industrial MSMEs in India.'\n\n"
        "Use accurate and concise responses based only on the provided context.\n"
        "Language should be clear and professional for plant operators, engineers, and MSME managers.\n\n"
        "Question: {question}\n\n"
        "Context: {context}\n\n"
        "Answer:"
    )
)

qa_chain = LLMChain(llm=llm, prompt=prompt_template)

# Function to answer questions using RAG
def answer_question_with_rag(question):
    retrieved_docs = retriever.get_relevant_documents(question)
    context = " ".join([doc.page_content for doc in retrieved_docs])
    response = qa_chain.run(question=question, context=context)
    formatted = clean_and_format_response(response)
    return formatted

import re

def clean_and_format_response(response):
    # Step 1: Remove any asterisks
    response = re.sub(r'\*+', '', response)

    # Step 2: Ensure that numbered points (e.g., 1., 2., etc.) are always on a new line
    # This regex finds numbers at the start or mid-sentence that represent list points
    response = re.sub(r'(?<!\n)(?<!\d)(\s*)(\d+)\.\s+', r'\n\2. ', response)

    # Step 3: Clean multiple newlines or spaces
    response = re.sub(r'\n{3,}', '\n\n', response)  # no more than 2 consecutive newlines
    response = re.sub(r'[ \t]+\n', '\n', response)  # remove trailing spaces before newlines
    response = response.strip()

    return response

query = "What is PAT Scheme?"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "How does the PAT Scheme benefit MSMEs?"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "Who is eligible for the Credit Linked Capital Subsidy Scheme (CLCSS) in the context of energy efficiency?"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "What is the capital subsidy offered under the TEQUP scheme for MSMEs?"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "Explain the bemefits of installing rooftop solar panels in MSME"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "What is UDAY and explain the incentives given by it?"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "Recommend me a scheme for msme and give its incentives"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "Recommend good practices for msme"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "What is the PAT scheme and how does it work?"
response = answer_question_with_rag(query)
print("Answer:", response)

query = "What are the main objectives of the PAT scheme?"
response = answer_question_with_rag(query)
print("Answer:", response)

def answer_energy_schemes_question_with_rag(question):
    # Replace this with your real function
    return answer_question_with_rag(question)

# Get user input (in Colab this opens an input box)
query = input("Enter your question: ")

# Get the answer
answer = answer_energy_schemes_question_with_rag(query)

print("\nAnswer:")
print(answer)