# -*- coding: utf-8 -*-
"""IDP GenAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14c-4ldBXAQ5-SW-j1Kli2q0sWGjEPWwV
"""

!pip install -q langchain-groq langchain chromadb sentence-transformers unstructured
!pip install -U langchain-community
!pip install -U langchain
!pip install -U httpx
!pip install groq

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

from langchain_groq import ChatGroq
import os

# Setup for Groq API key
os.environ["GROQ_API_KEY"] = "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"
groq_api_key = os.getenv("GROQ_API_KEY")

# Initialize LLM with low temperature
llm = ChatGroq(
    api_key=groq_api_key,
    model_name="llama3-8b-8192",
    temperature=0.2  # üîΩ Lower = more consistent, deterministic
)

!pip install pymupdf

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os

# --- Configuration ---
pdf_file_path = "/content/drive/MyDrive/RS775-4538_Datasheet.pdf"
persist_directory = "/content/drive/MyDrive/RS775-4538_Datasheet_DB_RS775"
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"

# --- Embedding Setup ---
embedding = HuggingFaceEmbeddings(model_name=embedding_model)

# --- Load PDF ---
loader = PyMuPDFLoader(pdf_file_path)
documents = loader.load()

# --- Split Text into Chunks ---
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)

print(f"‚úÖ Chunks created from RS775 datasheet: {len(chunks)}")

# --- Store in Chroma Vector DB ---
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embedding,
    persist_directory=persist_directory
)
vectorstore.persist()

print("‚úÖ RS775 Datasheet Vector DB created and saved.")

import shutil
from google.colab import files

# Path to your Chroma DB directory
persist_directory = "/content/drive/MyDrive/RS775-4538_Datasheet_DB_RS775"

# Output zip file name
zip_filename = "RS775_VectorDB.zip"

# Create zip file
shutil.make_archive("RS775_VectorDB", 'zip', persist_directory)

# Download zip file in Colab
files.download(zip_filename)

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from google.colab import files
import zipfile
import os

# 1Ô∏è‚É£ Upload the zipped vector DB (uploaded manually by user)
uploaded = files.upload()

# 2Ô∏è‚É£ Extract the uploaded zip file
zip_name = list(uploaded.keys())[0]
unzip_path = "/content/uploaded_vectordb"

with zipfile.ZipFile(zip_name, 'r') as zip_ref:
    zip_ref.extractall(unzip_path)

# 3Ô∏è‚É£ Load the embedding model
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 4Ô∏è‚É£ Load vectorstore from unzipped path
vectorstore = Chroma(
    persist_directory=unzip_path,
    embedding_function=embedding
)

# 5Ô∏è‚É£ Create retriever
retriever = vectorstore.as_retriever()

print("‚úÖ Vector DB loaded successfully from uploaded zip.")

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# ‚úÖ Updated prompt template for datasheet-based answers
prompt_template = PromptTemplate(
    input_variables=["question", "context"],
    template=(
        "You are an AI assistant that helps engineers understand technical information from motor datasheets. "
        "Use the provided context to answer the question accurately. Stick to the content of the RS775-4538 motor datasheet. "
        "If the answer is not available in the context, say you don't know.\n\n"
        "Context:\n{context}\n\n"
        "Question: {question}\n\n"
        "Answer:"
    )
)

# ‚úÖ Initialize the LLM chain using the updated prompt
qa_chain = LLMChain(
    llm=llm,  # e.g., OpenAI model or HuggingFace LLM
    prompt=prompt_template
)

# ‚úÖ Function to answer datasheet questions using RAG
def answer_motor_datasheet_question_with_rag(question):
    # Retrieve the relevant chunks from Chroma
    retrieved_docs = retriever.get_relevant_documents(question)

    # Combine content from retrieved documents
    context = " ".join([doc.page_content for doc in retrieved_docs])

    # Run the LLM chain with both context and question
    response = qa_chain.run(question=question, context=context)

    return response

response = answer_motor_datasheet_question_with_rag("What is the stall torque of the RS775 motor?")
print(response)

response = answer_motor_datasheet_question_with_rag("What is the maximum continuous current the motor can handle safely?")
print(response)

response = answer_motor_datasheet_question_with_rag("What are the recommended operating practices?")
print(response)

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# üß† Prompt Template for Generating Motor Issue Recommendations
prompt_template = PromptTemplate(
    input_variables=["issue", "context"],
    template=(
        "You are an AI assistant with expertise in electrical engineering and motor systems. "
        "Based on the following datasheet context for the RS775-4538 DC motor and the issue described, "
        "provide a clear, practical recommendation for resolving or mitigating the issue. "
        "Base your reasoning on the datasheet when possible. If not covered, use standard best practices. "
        "Do NOT invent technical specs that are not in the datasheet.\n\n"
        "Datasheet Context:\n{context}\n\n"
        "Observed Issue:\n{issue}\n\n"
        "Recommended Action:"
    )
)

# ‚úÖ Initialize the LLM chain with the updated recommendation prompt
recommendation_chain = LLMChain(
    llm=llm,  # HuggingFaceHub, ChatOpenAI, etc.
    prompt=prompt_template
)

# Function that generates a recommendation based on an alert/issue
def get_motor_issue_recommendation(issue_description):
    # Retrieve relevant chunks from vector DB using the issue
    retrieved_docs = retriever.get_relevant_documents(issue_description)

    # Merge retrieved datasheet content into a single context string
    context = " ".join([doc.page_content for doc in retrieved_docs])

    # Generate a recommendation
    recommendation = recommendation_chain.run(issue=issue_description, context=context)

    return recommendation

issue = "Overheating: temperature 72.85¬∞C exceeds 70.0¬∞C"
print(get_motor_issue_recommendation(issue))

issue = "Voltage drop: V=19.6V below 20.0V"
print(get_motor_issue_recommendation(issue))

issue = "Current spike: ŒîI=2.20A (I=2.49A)"
print(get_motor_issue_recommendation(issue))

issue = "Efficiency low: 73.9%"
print(get_motor_issue_recommendation(issue))

issue = "Average power: 56.9 W, Peak power: 104.3 W"
print(get_motor_issue_recommendation(issue))

"""# Task
Provide instructions on how to run the current Colab notebook locally, including downloading the notebook, installing dependencies, setting up environment variables, adjusting file paths, and running the notebook.

## Download the notebook

### Subtask:
Download the current Colab notebook to your local machine.

## Install dependencies

### Subtask:
Install all the necessary libraries and dependencies locally using pip.

**Reasoning**:
Install all the required libraries using pip.
"""

!pip install langchain-groq langchain chromadb sentence-transformers unstructured langchain-community httpx groq pymupdf